# Hadoop Incident Triage (POC)

POC Streamlit pour analyser des **logs publics Hadoop (HDFS / YARN)** avec un **LLM local (Ollama)**.

Objectifs principaux‚ÄØ:

- Charger des logs Hadoop (HDFS / YARN) publics
- Calculer des **signaux rapides** (heuristiques simples mais utiles)
- Construire une **timeline** (bursts d'√©v√©nements par minute)
- Faire une **recherche RAG** (FAISS) pour extraire des extraits pertinents
- Appliquer une logique **multi‚Äëagents** :
  1. Agent HDFS : expert HDFS (replication, DataNode, IO, NameNode)
  2. Agent YARN : expert YARN (RM/NM, containers, ressources, blacklisting)
  3. Agent Reviewer : ¬´‚ÄØprincipal SRE‚ÄØ¬ª qui challenge et consolide
- Exporter un **rapport RCA** en Markdown

> üîí **Compliance** : tout est local (LLM via Ollama) et les logs utilis√©s doivent √™tre **publics**.

---

## 1. Pr√©requis

- Python 3.9+ install√© sur la machine
- [Git](https://git-scm.com/) + Git Bash (sous Windows)
- [Ollama](https://ollama.com/) install√© et d√©marr√©
- Acc√®s au mod√®le Ollama **`qwen2.5:1.5b`**

Sous Windows, le projet est pens√© pour fonctionner via **Git Bash** (par exemple dans Wave Terminal).

Les jeux de logs Hadoop utilis√©s pour le POC peuvent, par exemple, √™tre extraits depuis le projet **[Loghub](https://github.com/logpai/loghub)** au format texte brut. Un script d‚Äôaide est fourni dans le r√©pertoire `scripts/` pour automatiser le t√©l√©chargement / l‚Äôextraction de certains jeux de logs publics.

---

## 2. Installation

Clone du repo puis installation locale :

```bash
git clone https://github.com/TON_ORG/hadoop-incident-triage.git
cd hadoop-incident-triage
```

### 2.1. Cr√©er l‚Äôenvironnement virtuel

Sous Git Bash / Linux / macOS :

```bash
python3 -m venv .venv
```

> Sur Windows (Git Bash), l‚Äôex√©cutable Python du venv est dans `./.venv/Scripts/python.exe`.

### 2.2. Activer le venv et installer les d√©pendances

Sous Git Bash :

```bash
source .venv/Scripts/activate   # Windows + Git Bash
# source .venv/bin/activate     # Linux / macOS

python -m pip install -U pip
python -m pip install -r requirements.txt
```

### 2.3. T√©l√©charger le mod√®le Ollama

Assure‚Äëtoi qu‚ÄôOllama est d√©marr√©, puis :

```bash
ollama pull qwen2.5:1.5b
```

---

## 3. Lancer l‚Äôapplication Streamlit

Avec le venv **activ√©** :

```bash
streamlit run app.py
```

Ou, si tu pr√©f√®res appeler explicitement l‚Äôex√©cutable du venv :

```bash
.venv/Scripts/streamlit.exe run app.py      # Windows + Git Bash
# .venv/bin/streamlit run app.py           # Linux / macOS
```

Streamlit t‚Äôindiquera l‚ÄôURL locale, typiquement :

- http://localhost:8501

---

## 4. Utilisation de l‚ÄôUI

### 4.1. R√©glages (barre lat√©rale)

- **Mod√®le Ollama** : par d√©faut `qwen2.5:1.5b`
- **Chunk size** : taille des chunks pour le RAG (ex. 600 caract√®res)
- **Chunk overlap** : recouvrement entre chunks
- **TopK RAG** : nombre d‚Äôextraits les plus pertinents √† r√©cup√©rer
- **Limite chars / fichier (RAM)** : coupe les fichiers trop volumineux pour rester l√©ger

### 4.2. √âtapes d‚Äôanalyse

1. **Charger des logs**  
   - Drag & drop un ou plusieurs fichiers `.log`, `.txt`, `.md` (HDFS / YARN, logs publics).
2. **Signaux rapides + timeline**  
   - L‚Äôapp calcule :
     - nombre total de lignes
     - mots‚Äëcl√©s fr√©quents (`error`, `warn`, `exception`, `replica`, `container`, etc.)
     - ¬´ composants ¬ª na√Øfs (token avant `:` ou `[pid]`)
   - Une **timeline par minute** est affich√©e si des timestamps au format `YYYY-MM-DD HH:MM` sont d√©tect√©s.
3. **Analyse LLM (multi‚Äëagents)**  
   - Bouton **¬´ Lancer l‚Äôanalyse ¬ª** :
     - Indexation RAG des logs dans FAISS
     - Agent HDFS : analyse des sympt√¥mes c√¥t√© HDFS
     - Agent YARN : analyse des sympt√¥mes c√¥t√© YARN
     - Agent Reviewer : produit une **RCA consolid√©e** en fran√ßais
4. **Export du rapport**  
   - Bouton **¬´ T√©l√©charger RCA_HADOOP_POC.md ¬ª** : export Markdown contenant :
     - signaux rapides (JSON)
     - timeline (JSON)
     - analyse HDFS
     - analyse YARN
     - RCA consolid√©e

---

## 5. Architecture technique

- **Frontend** : [Streamlit](https://streamlit.io/)
- **LLM** : [Ollama](https://ollama.com/) + mod√®le local `qwen2.5:1.5b`
- **RAG** :
  - D√©coupage des logs via `RecursiveCharacterTextSplitter`
  - Vectorisation avec `FastEmbedEmbeddings` (mod√®le `BAAI/bge-small-en-v1.5`)
  - Indexation / recherche avec **FAISS** (`langchain_community.vectorstores.FAISS`)
- **Multi‚Äëagents** :
  - 3 appels LLM avec des prompts sp√©cialis√©s (HDFS, YARN, Reviewer)

---

## 6. Limitations & notes

- POC uniquement : pas pr√©vu pour de la prod telle‚Äëquelle.
- Les heuristiques de **signaux rapides** et de **timeline** sont volontairement simples.
- Les logs doivent √™tre **anonymis√©s / publics**. Ne pas utiliser de donn√©es sensibles.
- Performances et qualit√© de l‚Äôanalyse d√©pendent :
  - du mod√®le local (taille, capacit√© de raisonnement),
  - de la quantit√©/taille des logs fournis.

---

## 7. D√©pannage (FAQ rapide)

### `ModuleNotFoundError: No module named 'langchain_text_splitters'`

V√©rifie que les d√©pendances sont bien install√©es dans le bon venv :

```bash
source .venv/Scripts/activate
python -m pip install -r requirements.txt
```

### `ollama: command not found` ou probl√®me de mod√®le

- V√©rifie qu‚ÄôOllama est install√© et lanc√©.
- V√©rifie que `ollama` est dans le `PATH`.
- V√©rifie que le mod√®le `qwen2.5:1.5b` est bien pr√©sent :

  ```bash
  ollama list
  ```

---

## 8. Licence

√Ä compl√©ter avec la licence souhait√©e (MIT, Apache 2.0, etc.).
